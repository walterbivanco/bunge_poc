"""
FastAPI Application - NL to SQL Chatbot
Orquesta todo el flujo: recibe pregunta ‚Üí genera SQL ‚Üí ejecuta ‚Üí retorna resultados
"""
import os
import time
import uuid
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from dotenv import load_dotenv

from app.models import AskRequest, AskResponse, ErrorResponse, HealthResponse
from app.prompts import get_prompt
from app.llm import init_vertex_ai, nl_to_sql, recommend_chart_type
from app.db import get_table_schema, get_dimensions_info, execute_query, test_connection
from app.logger import metrics_collector, log_info, log_error, log_warning

# Cargar variables de entorno
load_dotenv()

# Inicializar Vertex AI al arrancar la aplicaci√≥n
log_info("Iniciando aplicaci√≥n NL ‚Üí SQL Chatbot")
try:
    init_vertex_ai()
    log_info("Vertex AI inicializado correctamente")
except Exception as e:
    log_warning(f"No se pudo inicializar Vertex AI: {e}")

# Crear aplicaci√≥n FastAPI
app = FastAPI(
    title="NL to SQL Chatbot",
    description="Chatbot que convierte lenguaje natural a SQL usando Gemini y BigQuery",
    version="1.0.0"
)

# Configurar CORS para permitir requests desde el frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En producci√≥n, especificar dominios espec√≠ficos
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/", include_in_schema=False)
async def root():
    """Servir el frontend construido"""
    # Intentar servir desde dist (build de producci√≥n)
    dist_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "frontend", "dist", "index.html")
    if os.path.exists(dist_path):
        return FileResponse(dist_path)
    # Fallback a index.html del frontend (desarrollo)
    frontend_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "frontend", "index.html")
    return FileResponse(frontend_path)


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    Endpoint de health check
    Verifica que BigQuery y Vertex AI est√©n funcionando
    """
    bigquery_ok = test_connection()
    vertex_ai_ok = os.getenv("PROJECT_ID") is not None
    
    status = "healthy" if (bigquery_ok and vertex_ai_ok) else "degraded"
    
    return HealthResponse(
        status=status,
        bigquery=bigquery_ok,
        vertex_ai=vertex_ai_ok
    )


@app.post("/ask", response_model=AskResponse, responses={500: {"model": ErrorResponse}})
async def ask_question(request: AskRequest):
    """
    Endpoint principal: recibe pregunta en lenguaje natural y retorna resultados
    
    Flujo:
    1. Obtiene el schema de la tabla BigQuery
    2. Construye el prompt con contexto
    3. Llama a Gemini para generar SQL
    4. Ejecuta el SQL en BigQuery
    5. Retorna resultados formateados
    """
    # Generar ID √∫nico para este request
    request_id = str(uuid.uuid4())[:8]
    start_time = time.time()
    steps = []
    
    log_info(f"üîµ Nuevo request [{request_id}]: {request.question}")
    
    try:
        # 1. Obtener schema de la tabla
        step_start = time.time()
        log_info(f"[{request_id}] Paso 1: Obteniendo schema de BigQuery")
        
        project_id = os.getenv("PROJECT_ID")
        dataset = os.getenv("BQ_DATASET")
        table = os.getenv("BQ_TABLE")
        
        if not all([project_id, dataset, table]):
            raise HTTPException(
                status_code=500,
                detail="Configuraci√≥n incompleta: faltan PROJECT_ID, BQ_DATASET o BQ_TABLE"
            )
        
        schema_text, table_full_id = get_table_schema()
        step_duration = (time.time() - step_start) * 1000
        steps.append({"name": "Get Schema", "duration_ms": step_duration})
        log_info(f"[{request_id}] Schema obtenido ({step_duration/1000:.2f}s)")
        
        # 1.5. Obtener informaci√≥n de dimensiones (si est√° disponible)
        dimensions_info = None
        try:
            step_start_dim = time.time()
            log_info(f"[{request_id}] Obteniendo informaci√≥n de tablas de dimensiones...")
            dimensions_info = get_dimensions_info()
            step_duration_dim = (time.time() - step_start_dim) * 1000
            if dimensions_info.get("dimensions"):
                log_info(f"[{request_id}] {len(dimensions_info['dimensions'])} tablas de dimensiones cargadas ({step_duration_dim/1000:.2f}s)")
        except Exception as e:
            log_warning(f"[{request_id}] No se pudieron cargar dimensiones (continuando sin ellas): {e}")
            dimensions_info = None
        
        # 2. Construir prompt
        step_start = time.time()
        log_info(f"[{request_id}] Paso 2: Construyendo prompt")
        
        prompt = get_prompt(
            question=request.question,
            schema=schema_text,
            project_id=project_id,
            dataset=dataset,
            table=table,
            dimensions_info=dimensions_info
        )
        step_duration = (time.time() - step_start) * 1000
        steps.append({"name": "Build Prompt", "duration_ms": step_duration})
        log_info(f"[{request_id}] Prompt construido ({step_duration/1000:.3f}s, {len(prompt)} chars)")
        
        # 3. Generar SQL con Gemini
        log_info(f"[{request_id}] Paso 3: Generando SQL con Gemini")
        llm_result = nl_to_sql(prompt)
        sql = llm_result['sql']
        steps.append({"name": "Generate SQL (Gemini)", "duration_ms": llm_result['duration_ms']})
        
        # 4. Ejecutar SQL en BigQuery
        step_start = time.time()
        log_info(f"[{request_id}] Paso 4: Ejecutando SQL en BigQuery")
        bq_result = execute_query(sql)
        step_duration = (time.time() - step_start) * 1000
        steps.append({"name": "Execute SQL (BigQuery)", "duration_ms": step_duration})
        log_info(f"[{request_id}] BigQuery completado ({step_duration/1000:.2f}s)")
        
        # 5. Analizar datos y recomendar tipo de gr√°fico
        chart_recommendation = None
        if bq_result["total_rows"] > 0 and bq_result["total_rows"] <= 100:
            step_start = time.time()
            log_info(f"[{request_id}] Paso 5: Analizando datos para recomendaci√≥n de gr√°fico")
            try:
                chart_recommendation = recommend_chart_type(
                    question=request.question,
                    columns=bq_result["columns"],
                    rows=bq_result["rows"]
                )
                step_duration = (time.time() - step_start) * 1000
                steps.append({"name": "Chart Recommendation", "duration_ms": step_duration})
                log_info(f"[{request_id}] Recomendaci√≥n de gr√°fico: {chart_recommendation.get('chart_type', 'none')}")
            except Exception as e:
                log_warning(f"[{request_id}] Error al recomendar gr√°fico: {e}")
                chart_recommendation = {"chart_type": None, "chart_config": None}
        
        # Calcular tiempo total
        total_time_ms = (time.time() - start_time) * 1000
        
        # Log de m√©tricas
        metrics_collector.log_request({
            "request_id": request_id,
            "question": request.question,
            "steps": steps,
            "total_time_ms": total_time_ms,
            "sql": sql,
            "rows_returned": bq_result["total_rows"],
            "tokens_used": llm_result.get('tokens_used'),
            "model_used": llm_result.get('model_used'),
            "bytes_processed": bq_result.get('bytes_processed'),
            "success": True
        })
        
        # 6. Retornar respuesta
        log_info(f"‚úÖ Request [{request_id}] completado exitosamente en {total_time_ms/1000:.2f}s")
        
        return AskResponse(
            question=request.question,
            sql=sql,
            columns=bq_result["columns"],
            rows=bq_result["rows"],
            total_rows=bq_result["total_rows"],
            chart_type=chart_recommendation.get("chart_type") if chart_recommendation else None,
            chart_config=chart_recommendation.get("chart_config") if chart_recommendation else None
        )
        
    except ValueError as e:
        total_time_ms = (time.time() - start_time) * 1000
        log_error(f"[{request_id}] Error de validaci√≥n", e)
        
        metrics_collector.log_request({
            "request_id": request_id,
            "question": request.question,
            "steps": steps,
            "total_time_ms": total_time_ms,
            "success": False,
            "error": str(e)
        })
        
        raise HTTPException(status_code=400, detail=str(e))
        
    except Exception as e:
        total_time_ms = (time.time() - start_time) * 1000
        log_error(f"[{request_id}] Error procesando pregunta", e)
        
        metrics_collector.log_request({
            "request_id": request_id,
            "question": request.question,
            "steps": steps,
            "total_time_ms": total_time_ms,
            "success": False,
            "error": str(e)
        })
        
        raise HTTPException(
            status_code=500,
            detail=f"Error procesando la pregunta: {str(e)}"
        )


@app.get("/schema")
async def get_schema(refresh: bool = False):
    """
    Endpoint para obtener el schema de la tabla configurada y tablas de dimensiones
    
    Query params:
        - refresh: Si es True, fuerza recarga del schema (ignora cach√©)
    """
    try:
        schema_text, table_id = get_table_schema(use_cache=not refresh)
        cached_str = " (sin cach√©)" if refresh else " (cach√©)"
        log_info(f"Schema solicitado para tabla: {table_id}{cached_str}")
        
        # Intentar obtener informaci√≥n de dimensiones
        dimensions_info = None
        try:
            dimensions_info = get_dimensions_info(use_cache=not refresh)
        except Exception as e:
            log_warning(f"No se pudieron cargar dimensiones: {e}")
        
        result = {
            "table": table_id,
            "schema": schema_text
        }
        
        if dimensions_info:
            result["dimensions"] = dimensions_info
        
        return result
    except Exception as e:
        log_error("Error obteniendo schema", e)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/metrics")
async def get_metrics():
    """
    Endpoint para obtener m√©tricas y estad√≠sticas del sistema
    """
    log_info("M√©tricas solicitadas")
    try:
        stats = metrics_collector.get_stats()
        recent = metrics_collector.get_recent_metrics(limit=10)
        
        return {
            "stats": stats,
            "recent_requests": recent
        }
    except Exception as e:
        log_error("Error obteniendo m√©tricas", e)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/logs")
async def get_logs(lines: int = 50):
    """
    Endpoint para obtener los √∫ltimos logs
    """
    log_info(f"Logs solicitados (√∫ltimas {lines} l√≠neas)")
    try:
        import os
        log_file = "chatbot.log"
        
        if not os.path.exists(log_file):
            return {"logs": [], "message": "No hay logs disponibles a√∫n"}
        
        with open(log_file, 'r') as f:
            all_lines = f.readlines()
            recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
            
        return {
            "logs": [line.strip() for line in recent_lines],
            "total_lines": len(all_lines),
            "showing": len(recent_lines)
        }
    except Exception as e:
        log_error("Error obteniendo logs", e)
        raise HTTPException(status_code=500, detail=str(e))


# Montar archivos est√°ticos del frontend construido
frontend_dist = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "frontend", "dist")
frontend_src = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "frontend")

# Rutas de API que no deben ser capturadas por el SPA
API_ROUTES = ["/ask", "/health", "/schema", "/metrics", "/logs", "/docs", "/openapi.json", "/redoc"]

# Priorizar dist (build de producci√≥n) si existe
if os.path.exists(frontend_dist):
    # Montar assets est√°ticos
    assets_dir = os.path.join(frontend_dist, "assets")
    if os.path.exists(assets_dir):
        app.mount("/assets", StaticFiles(directory=assets_dir), name="assets")
    
    # Servir otros archivos est√°ticos del build y manejar routing de React
    @app.get("/{path:path}", include_in_schema=False)
    async def serve_spa(path: str):
        """Servir archivos del SPA o redirigir a index.html para routing de React"""
        # No interceptar rutas de API
        if path in API_ROUTES or path.startswith(tuple(API_ROUTES)):
            raise HTTPException(status_code=404, detail="Not found")
        
        full_path = os.path.join(frontend_dist, path)
        if os.path.exists(full_path) and os.path.isfile(full_path):
            return FileResponse(full_path)
        # Si no existe, servir index.html para que React Router maneje la ruta
        return FileResponse(os.path.join(frontend_dist, "index.html"))
else:
    # Fallback: servir desde el directorio fuente (desarrollo)
    app.mount("/static", StaticFiles(directory=frontend_src), name="static")


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8080))
    log_info(f"Iniciando servidor en puerto {port}")
    uvicorn.run(app, host="0.0.0.0", port=port)

